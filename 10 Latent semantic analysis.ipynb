{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3a4b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8ae79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    \"Latent semantic analysis is a technique in natural language processing.\",\n",
    "    \"It is used to discover hidden patterns in a set of documents.\",\n",
    "    \"LSA can be applied to various text analysis tasks, such as information retrieval and document classification.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b131b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Yuva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "# Tokenize the documents\n",
    "tokenized_documents = [nltk.word_tokenize(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff32c79a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x32 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 36 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2148c385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70391338, -0.25193762],\n",
       "       [ 0.69552698, -0.30305683],\n",
       "       [ 0.43025184,  0.90209134]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform Truncated Singular Value Decomposition (SVD) for LSA\n",
    "num_topics = 2  # You can adjust the number of topics as needed\n",
    "lsa = TruncatedSVD(n_components=num_topics)\n",
    "lsa_matrix = lsa.fit_transform(tfidf_matrix)\n",
    "lsa_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bbc4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics:\n",
      "Topic 1: in is analysis to semantic\n",
      "Topic 2: applied various lsa and text\n"
     ]
    }
   ],
   "source": [
    "# Print the topics\n",
    "print(\"Topics:\")\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lsa.components_):\n",
    "    top_terms = [terms[idx] for idx in topic.argsort()[-5:][::-1]]\n",
    "    print(f\"Topic {i + 1}: {' '.join(top_terms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c5bb066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Topic Matrix:\n",
      "Document 1: [ 0.70391338 -0.25193762]\n",
      "Document 2: [ 0.69552698 -0.30305683]\n",
      "Document 3: [0.43025184 0.90209134]\n"
     ]
    }
   ],
   "source": [
    "# Print the document-topic matrix\n",
    "print(\"Document-Topic Matrix:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i + 1}: {lsa_matrix[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636ff2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Similarity:\n",
      "Similarity between Document 1 and Document 2: 0.9977420219483246\n",
      "Similarity between Document 1 and Document 3: 0.10116014908584053\n",
      "Similarity between Document 2 and Document 3: 0.03411334846122621\n"
     ]
    }
   ],
   "source": [
    "# Calculate the similarity between documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Document Similarity:\")\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i + 1, len(documents)):\n",
    "        similarity = cosine_similarity(lsa_matrix[i].reshape(1, -1), lsa_matrix[j].reshape(1, -1))\n",
    "        print(f\"Similarity between Document {i + 1} and Document {j + 1}: {similarity[0][0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
